{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f880ad",
   "metadata": {},
   "source": [
    "# 2. Allocation Model (Decision Engine)\n",
    "Adapts training logic from train.py to an allocation decision engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea68d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from src.allocation_model import ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76d859",
   "metadata": {},
   "source": [
    "### Model setup & training (refactored from original project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e82829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a new network on a data set with train.py\n",
    "\n",
    "Basic usage: python train.py data_directory\n",
    "Prints out training loss, validation loss, and validation accuracy as the network trains\n",
    "Options:\n",
    "Set directory to save checkpoints:    python train.py data_dir --save_dir save_directory\n",
    "Choose architecture:                  python train.py data_dir --arch \"vgg13\"\n",
    "Set hyperparameters:                  python train.py data_dir --learning_rate 0.01 --hidden_units 512 --epochs 20\n",
    "Use GPU for training:                 python train.py data_dir --gpu\n",
    "\"\"\"\n",
    "\n",
    "# TODO: IMPORTS HERE\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import argparse\n",
    "\n",
    "#######################################################################################################################\n",
    "# TODO: Add command line arguments\n",
    "\n",
    "# Set default values for all future arguments\n",
    "default_hidden_units = [1024, 512]\n",
    "default_learning_rate = 0.003\n",
    "default_epochs = 5\n",
    "default_arch = 'densenet161'\n",
    "default_device = 'gpu'\n",
    "\n",
    "# Creates Argument Parser object called parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# TODO: Arguments for inputs\n",
    "parser.add_argument('data_dir', type=str, help='Location of directory with data for training')\n",
    "parser.add_argument('--save_dir', type=str, help='Set directory to save checkpoint')\n",
    "parser.add_argument('--arch', type=str, default=default_arch,\n",
    "                    help='Select a pretrained networks: \"densenet161\" or \"alexnet\"')\n",
    "parser.add_argument('--hidden_units', type=int, default=default_hidden_units, help='List of hidden units'\n",
    "                                                                                   'of model (max 2)')\n",
    "parser.add_argument('--learning_rate', type=float, default=default_learning_rate,\n",
    "                    help='Set learning rate hyperparameter (float)')\n",
    "parser.add_argument('--epochs', type=int, default=default_epochs, help='Set epochs hyperparameter (int)')\n",
    "parser.add_argument('--gpu', type=str, default=default_device, help='Uses \"GPU\" if available')\n",
    "\n",
    "# Assign variable in parse_args() to access the arguments in the argparse object\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Assign variables to use in the model\n",
    "data_dir = args.data_dir\n",
    "save_dir = args.save_dir\n",
    "arch = args.arch\n",
    "hidden_units = args.hidden_units\n",
    "learning_rate = args.learning_rate\n",
    "epochs_train = args.epochs\n",
    "device = args.gou\n",
    "\n",
    "#######################################################################################################################\n",
    "# TODO: DEFINE THE MODEL\n",
    "\n",
    "# Use getattr() f'n to get the arch model from Class 'models'\n",
    "model = getattr(models, arch)(pretrained=True)\n",
    "\n",
    "## Freeze 'feature' parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# TODO: Define the input size, with the output fixed at 102\n",
    "if arch == 'densenet161':\n",
    "    input_size = 2208\n",
    "elif arch == 'alexnet':\n",
    "    input_size = 9216\n",
    "else:\n",
    "    print('Error, unexpected architecture set')\n",
    "    exit()\n",
    "\n",
    "# TODO: Define Model Architecture\n",
    "model.classifier = nn.Sequential(nn.Linear(input_size, hidden_units[0]),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.15),\n",
    "                                 nn.Linear(hidden_units[0], hidden_units[1]),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(hidden_units[1], 102),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define criterion for the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Define optimizer for backpropagation\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move weights to memory of the active device (GPU/CPU)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model successfully defined\")\n",
    "\n",
    "#######################################################################################################################\n",
    "# TODO:  DEFINE TRAINING AND VALIDATION DATALOADERS\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'\n",
    "\n",
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(43),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.3),\n",
    "                                       transforms.RandomVerticalFlip(p=0.1),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=36, shuffle=True)\n",
    "validatorloader = torch.utils.data.DataLoader(valid_dataset, batch_size=36)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=36)\n",
    "\n",
    "print(\"3 sets of datasets successfully created: Training, Validation (during training), Testing\")\n",
    "\n",
    "#######################################################################################################################\n",
    "# TODO: TRAIN NETWORK and VALIDATE\n",
    "\n",
    "# Set epochs hyperparameter & Initial running loss\n",
    "epochs = epochs_train\n",
    "running_loss = 0\n",
    "\n",
    "# Initialize # of steps for Eval check\n",
    "steps = 0\n",
    "print_every = 50\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"For loop 1: {e} and {epochs}\")\n",
    "\n",
    "    # Training for pass for each epoch\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        # Keep track of 'steps' for testing 'if' clause\n",
    "        steps += 1\n",
    "\n",
    "        # Move images, labels tensors to 'device' memory\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Clean accumulated gradients from last pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, loss calc, backward pass, update weights\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Check periodically to run validation pass, in the middle\n",
    "        if steps % print_every == 0:\n",
    "\n",
    "            # Initialize test loss and accuracy variables\n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            # Turn off gradients for Eval pass of the model\n",
    "            model.eval()\n",
    "\n",
    "            # Turn off gradients for Eval pass of the model\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Start Eval pass w/ its own set of images w/in the batch\n",
    "                for valid_img, valid_labels in validatorloader:\n",
    "                    # Move to valid_img, valid_labels tensors to 'device' memory\n",
    "                    valid_img, valid_labels = valid_img.to(device), valid_labels.to(device)\n",
    "\n",
    "                    # 1 forward pass, loss calc for the batch of images\n",
    "                    log_ps = model(valid_img)\n",
    "                    batch_loss = criterion(log_ps, valid_labels)\n",
    "\n",
    "                    valid_loss += batch_loss.item()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    # Convert log(softmax) prob to prob distribution\n",
    "                    ps = torch.exp(log_ps)\n",
    "\n",
    "                    # Get 'top_class' from 'topk' and equivocate the prediction\n",
    "                    # and valid_labels tensor (i.e. same dimensions)\n",
    "                    # 1 = 1st largest value in prob distribution\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == valid_labels.view(*top_class.shape)\n",
    "\n",
    "                    # Convert 'equals' to a FloatTensor, then calc mean\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "            # Print data on this Training and Validation pass\n",
    "            print(f\"Epoch {e + 1}/{epochs}.. \"\n",
    "                  f\"Training loss: {running_loss / print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {valid_loss / len(validatorloader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy / len(validatorloader):.3f}\")\n",
    "\n",
    "            # Reset running loss to 0\n",
    "            running_loss = 0\n",
    "\n",
    "            # Set model back to training mode\n",
    "            model.train()\n",
    "\n",
    "print(\"Model trained, please see above for statistics on the training\")\n",
    "\n",
    "#######################################################################################################################\n",
    "# TODO: SAVE CHECKPOINT\n",
    "\n",
    "model.class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "checkpoint = {'input_size': input_size,\n",
    "              'output_size': 102,\n",
    "              'hidden_units': hidden_units,\n",
    "              'network': arch,\n",
    "              'classifier': model.classifier,\n",
    "              'learning_rate': learning_rate,\n",
    "              'optimizer': optimizer.state_dict(),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'epochs': epochs_train,\n",
    "              'class_to_idx': model.class_to_idx\n",
    "              }\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "print(\"Checkpoint successfully saved\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
